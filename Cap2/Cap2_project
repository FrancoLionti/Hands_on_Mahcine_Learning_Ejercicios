import sys
assert sys.version_info >= (3, 7), "Requires Python >= 3.7"
from packaging import version
import sklearn
assert version.parse(sklearn.__version__) >= version.parse("1.0.1")

from pathlib import Path
import pandas as pd
import tarfile
import urllib.request
import matplotlib.pyplot as plt
import numpy as np
from zlib import crc32
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedShuffleSplit
# from scipy.stats import binom
from pandas.plotting import scatter_matrix

def load_housing_data():
    tarball_path = Path("datasets/housing.tgz")
    if not tarball_path.is_file():
        Path("datasets").mkdir(parents=True, exist_ok=True)
        url = "https://github.com/ageron/data/raw/main/housing.tgz"
        urllib.request.urlretrieve(url, tarball_path)
        with tarfile.open(tarball_path) as housing_tarball:
            housing_tarball.extractall(path="datasets")
    return pd.read_csv(Path("datasets/housing/housing.csv"))

housing = load_housing_data()

""" Printing values from housing

print(housing.head())
print(housing.describe().to_string())
housing.hist(bins=50, figsize=(12,8))
plt.show()
"""

""" Shuffle and split data is a method that uses numpy to shuffle and split 
the housing data array. This is done by using numpy random function, more 
specifically permutation which changes the indices of the array (positions).
"""

def shuffle_and_split_data(data,test_ratio):
    shuffled_instances=np.random.permutation(len(data)) # Permutes the indices
    test_set_size = int(len(data)*test_ratio) # The size of the test set
    test_indices = shuffled_instances[:test_set_size] # The indices of the test set
    train_indices = shuffled_instances[test_set_size:] # The indices of the train set
    return data.iloc[train_indices], data.iloc[test_indices] 

""" Test cases
train_set, test_set = shuffle_and_split_data(housing, 0.2)
len(train_set), len(test_set)
"""

""" 
is_id_in_test_set is a function that performs a ciclycal redundacy check 
by summing the ids of the passed array. It only returns True if the ids are 
less than 2^32 times the passed test_ratio.
"""

def is_id_in_test_set(identifier, test_ratio):
    return crc32(np.int64(identifier)) < test_ratio * 2**32

def split_data_with_id_hash(data, test_ratio, id_column):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_: is_id_in_test_set(id_, test_ratio))
    return data.loc[~in_test_set], data.loc[in_test_set]

""" 
I used this by the books example but in the end another more efective and 
preexistent implementation is suggested (I'll leave the functions untouched 
in the code so i can consult them if i need to get a glimpse of what the 
code is doing).

housing_with_id = housing.reset_index() # Adds an 'index' column
train_set, test_set = split_data_with_id_hash(housing, 0.2, "longitude")


housing_with_id["id"] = housing["longitude"] * 1000 + housing["latitude"]
train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, "id")
"""

train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)
test_set["total_bedrooms"].isnull().sum()

# print(test_set["total_bedrooms"].isnull().sum())

""" 
# Extra code - shows how to compute the 10.7% probability of getting 
# a bad sample:

sample_size = 1000
ratio_female = 0.511
proba_too_small = binom(sample_size, ratio_female).cdf(485 - 1)
proba_too_large = 1 - binom(sample_size, ratio_female).cdf(535)
print(proba_too_small + proba_too_large)

"""

housing["income_cat"]= pd.cut(housing["median_income"],
bins=[0,1.5,3,4.5,6,np.inf],labels=[1,2,3,4,5]) 
# Segments and sorts the data into bins

""" 
Generating a bar plot from the sorted array

housing["income_cat"].value_counts().sort_index().plot.bar(rot=0,grid=True) 

plt.xlabel("Income Category")
plt.ylabel("Number of districts")
plt.show()

"""

""" Stratified testing technique by the books example. Here is where i define
the pertinent strata for the current test set. This is done by segmentating 
the data by income"""

splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
strat_splits = []
for train_index, test_index in splitter.split(housing, housing["income_cat"]):
    strat_train_set_n = housing.iloc[train_index]
    strat_test_set_n = housing.iloc[test_index]
    strat_splits.append([strat_train_set_n, strat_test_set_n])

strat_train_set_n, strat_test_set_n = strat_splits[0]
strat_train_set_n, strat_test_set_n = train_test_split(housing, test_size=0.2, 
stratify=housing["income_cat"] ,random_state=42)

# print(strat_test_set_n["income_cat"].value_counts()/len(strat_test_set_n))

""" Since i wont be using the income_cat column again, 
i'll delete it from the current dataframe """

for set_ in (strat_train_set_n, strat_test_set_n):
    set_.drop("income_cat", axis=1, inplace=True)

""" Now i need to explore and visualize the current data. Therefore im going
to define a exploration set since the data is too big. Im going to do this by
just copying the dataframe. """

housing = strat_train_set_n.copy()

""" Plotting the data:
    Im going to generate the final example of the scatter plot from the 
    book's example. This takes into account latitude and longitude as well 
    as median house values in a predefined jet color map. 

    housing.plot(kind="scatter", x="longitude", y="latitude", grid=True,
            s=housing["population"] / 100, label="population",
            c="median_house_value", colorbar=True,
            legend=True, sharex=False, figsize=(10, 7))
plt.show()
"""

""" 
For some weird ass reason ocean proximity column interferes to find pearsons
correlation. I dropped said column. Error shown was of type conversion. 
"""

housing.drop("ocean_proximity", axis=1, inplace=True)

""" 
corr_matrix = housing.corr()    # Pearson's correlation.
corr_matrix["median_house_value"].sort_values(ascending=False) 
# Calculates how much each parameter correlates to median house value.
"""

""" 
Scatter plot matrix: This method plots every attribute in the passed 
array against every other numerical attribute.

attributes = ["median_house_value", "median_income", "total_rooms",
            "housing_median_age"]
scatter_matrix(housing[attributes], figsize=(12, 8))

plt.show()
"""

""" 
Income vs house value plot:

housing.plot(kind="scatter", x="median_income", y="median_house_value",
            alpha=0.1, grid=True)
plt.show() 
"""

""" 
Setting up new parameters (which are all attribute combinations) to see 
if they correlate with median house value.
"""

housing["rooms_per_house"] = housing["total_rooms"]/housing["households"]
housing["bedrooms_ratio"] = housing["total_bedrooms"]/housing["total_rooms"]
housing["people_per_house"] = housing["population"]/housing["households"]

# corr_matrix = housing.corr()
# print(corr_matrix["median_house_value"].sort_values(ascending=False))

""" Reverting to a clean training set: """

housing = strat_train_set_n.drop("median_house_value", axis=1)
housing_labels = strat_train_set_n["median_house_value"].copy()

