import sys
assert sys.version_info >= (3, 7), "Requires Python >= 3.7"
from packaging import version
import sklearn
assert version.parse(sklearn.__version__) >= version.parse("1.0.1")

from pathlib import Path
import pandas as pd
import tarfile
import urllib.request
import matplotlib.pyplot as plt
import numpy as np
from zlib import crc32
from sklearn.model_selection import train_test_split
from scipy.stats import binom

def load_housing_data():
    tarball_path = Path("datasets/housing.tgz")
    if not tarball_path.is_file():
        Path("datasets").mkdir(parents=True, exist_ok=True)
        url = "https://github.com/ageron/data/raw/main/housing.tgz"
        urllib.request.urlretrieve(url, tarball_path)
        with tarfile.open(tarball_path) as housing_tarball:
            housing_tarball.extractall(path="datasets")
    return pd.read_csv(Path("datasets/housing/housing.csv"))

housing = load_housing_data()

#Printing values from housing

# print(housing.head())
# print(housing.describe().to_string())
# housing.hist(bins=50, figsize=(12,8))
# plt.show()

""" Shuffle and split data is a method that uses numpy to shuffle and split the 
housing data array. This is done by using numpy random function, more specifically 
permutation which changes the indices of the array (positions).
"""

def shuffle_and_split_data(data,test_ratio):
    shuffled_instances=np.random.permutation(len(data)) # Permutes the indices
    test_set_size = int(len(data)*test_ratio) # The size of the test set
    test_indices = shuffled_instances[:test_set_size] # The indices of the test set
    train_indices = shuffled_instances[test_set_size:] # The indices of the train set
    return data.iloc[train_indices], data.iloc[test_indices] 

# Test cases
# train_set, test_set = shuffle_and_split_data(housing, 0.2)
# len(train_set), len(test_set)

""" 
is_id_in_test_set is a function that performs a ciclycal redundacy check 
by summing the ids of the passed array. It only returns True if the ids are 
less than 2^32 times the passed test_ratio.
"""

def is_id_in_test_set(identifier, test_ratio):
    return crc32(np.int64(identifier)) < test_ratio * 2**32

def split_data_with_id_hash(data, test_ratio, id_column):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_: is_id_in_test_set(id_, test_ratio))
    return data.loc[~in_test_set], data.loc[in_test_set]

""" 
I used this by the books example but in the end another more efective and preexistent 
implementation is suggested (I'll leave the functions untouched in the code so i can 
consult them if i need to get a glimpse of what the code is doing).

housing_with_id = housing.reset_index() # Adds an 'index' column
train_set, test_set = split_data_with_id_hash(housing, 0.2, "longitude")


housing_with_id["id"] = housing["longitude"] * 1000 + housing["latitude"]
train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, "id")
"""

train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)
test_set["total_bedrooms"].isnull().sum()

# print(test_set["total_bedrooms"].isnull().sum())

""" 
# Extra code - shows how to compute the 10.7% probability of getting 
# a bad sample:

sample_size = 1000
ratio_female = 0.511
proba_too_small = binom(sample_size, ratio_female).cdf(485 - 1)
proba_too_large = 1 - binom(sample_size, ratio_female).cdf(535)
print(proba_too_small + proba_too_large)

"""

housing["income_cat"]= pd.cut(housing["median_income"],
bins=[0,1.5,3,4.5,6,np.inf],labels=[1,2,3,4,5])

housing["income_cat"].value_counts().sort_index().plot.bar(rot=0,grid=True)
plt.xlabel("Income Category")
plt.ylabel("Number of districts")
plt.show()

